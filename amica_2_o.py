# -*- coding: utf-8 -*-
"""Amica 2.O.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12JgALMvju55C-ND-FcgJSve6zzE8wiK7
"""

!pip install sentence-transformers faiss-cpu transformers duckduckgo-search

!pip install duckduckgo-search

from duckduckgo_search import DDGS

def web_search(query):
    with DDGS() as ddgs:
        results = list(ddgs.text(query, max_results=3))
    if results:
        return results[0]["body"] if "body" in results[0] else results[0]["href"]
    else:
        return "Bro, net lo kuda information kanipinchaledu "

print(web_search("Who is the current CEO of Google?"))

from google.colab import files
uploaded = files.upload()



# Basic Imports
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from duckduckgo_search import DDGS


# Web search function
def web_search(query):
    with DDGS() as ddgs:
        results = list(ddgs.text(query, max_results=3))
    if results:
        return results[0]["body"] if "body" in results[0] else results[0]["href"]
    else:
        return "Bro, net lo kuda information kanipinchaledu"


# Load chat messages
def load_messages(path):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    messages = [entry["message"] for entry in data if "message" in entry and entry["sender"] == "Sarfaraz Ahamad"]
    return messages
#sarfaraz Ahamad is my friend where he helped me to collect the customized dataset of friendly tone

# Embed messages and build FAISS index
def build_index(messages):
    encoder = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = encoder.encode(messages, show_progress_bar=True)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    return encoder, index, embeddings

# Get similar responses
def retrieve_similar(query, encoder, index, messages, k=3):
    query_vec = encoder.encode([query])
    distances, indices = index.search(query_vec, k)
    return [messages[i] for i in indices[0]]

# Load open LLM
def load_llm():
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", use_auth_token=True)
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", device_map="auto")
    return pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=100)

# Final response generator
def generate_reply(query, encoder, index, messages, llm_pipeline):
    sims = retrieve_similar(query, encoder, index, messages)
    if sims and sims[0]:
        return f"Personal touch ü´Ç:\n{sims[0]}"
    else:
        # Web fallback
        web_info = web_search(query)
        if web_info:
            return f"Google se mila info üåê:\n{web_info}"
        # LLM fallback
        response = llm_pipeline(f"Tu kya bolega jab koi puche: {query}")
        return "Soch ke bola bhai:\n" + response[0]['generated_text']

"Ready to load your file and build the pipeline!"

import json

def load_messages(path="Chat.json"):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    messages = [d["message"] for d in data if "message" in d]
    return messages

messages = load_messages()
print("Sample:", messages[:4])

from sentence_transformers import SentenceTransformer
import numpy as np

encoder = SentenceTransformer("all-MiniLM-L6-v2")
message_embeddings = encoder.encode(messages, show_progress_bar=True)

import faiss

dimension = message_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(message_embeddings))

def get_similar_messages(query, k=3):
    query_embedding = encoder.encode([query])
    distances, indices = index.search(query_embedding, k)
    return [messages[i] for i in indices[0]]

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "HuggingFaceH4/zephyr-7b-alpha"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

from duckduckgo_search import DDGS

def web_search(query):
    with DDGS() as ddgs:
        results = list(ddgs.text(query, max_results=3))
    if results:
        return results[0]["body"] if "body" in results[0] else results[0]["href"]
    else:
        return "Bro, net lo kuda information kanipinchaledu"

def rewrite_in_sarfaraz_style(answer):
    """Rewrites any factual or generated response in Sarfaraz-style slang."""
    style_prompt = f"""
    Convert the following answer into casual Hindi-English slang like a funny and emotionally supportive friend (Sarfaraz style):

    Original: {answer}
    Styled:
    """
    response = generator(style_prompt, max_new_tokens=60, do_sample=True, temperature=0.9)[0]["generated_text"]
    return response.split("Styled:")[-1].strip()

def generate_response(user_input):
    similar = get_similar_messages(user_input)
    if similar:
        return f"Sarfaraz style lo cheppalante: {similar[0]}"

    # Web search fallback
    if any(q in user_input.lower() for q in ["what", "who", "when", "where", "how", "latest", "news"]):
        web_ans = web_search(user_input)
        return "Sarfaraz style lo cheppalante: " + rewrite_in_sarfaraz_style(web_ans)

    # Fallback to LLM and stylize
    prompt = f"Answer this in a factual way: {user_input}"
    raw_reply = generator(prompt, max_new_tokens=60, do_sample=True, temperature=0.7)[0]["generated_text"]
    return "Sarfaraz style lo cheppalante: " + rewrite_in_sarfaraz_style(raw_reply)

print(generate_response("Why are you ignoring me? üò¢"))

